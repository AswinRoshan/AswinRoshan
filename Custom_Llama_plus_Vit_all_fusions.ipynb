{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AswinRoshan/AswinRoshan/blob/main/Custom_Llama_plus_Vit_all_fusions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d14c61",
      "metadata": {
        "id": "02d14c61"
      },
      "outputs": [],
      "source": [
        "\"\"\" Import modules \"\"\"\n",
        "import torch\n",
        "# import time\n",
        "# from transformers import T5Tokenizer\n",
        "# from transformers import T5ForConditionalGeneration, AdamW\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/UoG Internship')\n",
        "# import re\n",
        "import cv2\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "# import torch.nn.functional as F\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "from datetime import datetime\n",
        "# storing the current time in the variable\n",
        "dt = datetime.now()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce893bf6",
      "metadata": {
        "id": "ce893bf6"
      },
      "source": [
        "\"\"\" set necessary environment variables \"\"\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134373a9",
      "metadata": {
        "id": "134373a9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load models for vision and text - ViT and llama\n",
        "Also load tokenizer\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f8d6bb",
      "metadata": {
        "id": "24f8d6bb"
      },
      "outputs": [],
      "source": [
        "double_quant_config = BitsAndBytesConfig(\n",
        "    # load_in_4bit=True,\n",
        "    # bnb_4bit_use_double_quant=True,\n",
        "    # bnb_4bit_compute_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba77ad0",
      "metadata": {
        "id": "6ba77ad0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"abhinand/tamil-llama-7b-base-v0.1\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"abhinand/tamil-llama-7b-base-v0.1\", low_cpu_mem_usage=True, quantization_config=double_quant_config, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23850aa2",
      "metadata": {
        "id": "23850aa2"
      },
      "outputs": [],
      "source": [
        "# from transformers import CLIPModel\n",
        "img_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc59b98a",
      "metadata": {
        "id": "cc59b98a"
      },
      "outputs": [],
      "source": [
        "\"\"\" set device \"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713b2437",
      "metadata": {
        "id": "713b2437"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "set dataset location\n",
        "folder - path containing tamil & malayalam folders\n",
        "\"\"\"\n",
        "folder = os.path.join(\"training_data\")\n",
        "language = \"tamil\"  # set accordingly\n",
        "dpth = os.path.join(folder, language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ab526b",
      "metadata": {
        "id": "93ab526b"
      },
      "outputs": [],
      "source": [
        "\"\"\" hyper-parameters \"\"\"\n",
        "N = 16       # batch-size\n",
        "T = 0.07     # temperature factor\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0fbb29",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "4e0fbb29"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Use for adding extra token - [PAD], for padding all to same length\n",
        "Later re-train the model for learning an embedding corresponding to the new vocab element\n",
        "\"\"\"\n",
        "tokenizer.all_special_ids\n",
        "special_tokens_dict = {\"pad_token\" : '[PAD]'}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tok_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
        "model.resize_token_embeddings(len(tokenizer))    # resizing embedding matrix of model to accomodate new [PAD]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c91af26",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3c91af26"
      },
      "outputs": [],
      "source": [
        "class LanData(Dataset):\n",
        "    def __init__(self, mode, dpth):\n",
        "        \"\"\"\n",
        "        Dataset class for loading images and transcriptions of memes\n",
        "        Add dataset path got above as parameter(dpth)\n",
        "        mode = train/test, to get training/testing dataset (saved as test and train folders in language folder)\n",
        "        \"\"\"\n",
        "\n",
        "        self.path = os.path.join(dpth, mode)   # mode = train/test\n",
        "        tscpts = os.path.join(self.path, mode + \".csv\")\n",
        "        self.csv = pd.read_csv(tscpts).dropna(axis = 0)\n",
        "        toks = tokenizer(self.csv.transcriptions.tolist(), truncation=True, max_length=75, padding = 'max_length', return_tensors = 'pt')\n",
        "        self.csv[\"tokens\"] = toks.input_ids.tolist()\n",
        "        self.csv[\"masks\"] = toks.attention_mask.tolist()\n",
        "\n",
        "        ims = []\n",
        "        labs = []\n",
        "        scpts = []\n",
        "        ms = []\n",
        "        imageIDs = []\n",
        "        mu = torch.zeros((1, 3, 1, 1))\n",
        "        sigma = torch.zeros((1, 3, 1, 1))\n",
        "\n",
        "        \"\"\"\n",
        "        Iterate over all images\n",
        "        Get mu (mean) and sigma (std-dev) of images\n",
        "        Normalize for better image pixel value distribution for better training\n",
        "        \"\"\"\n",
        "        tot = 0\n",
        "        for f in os.listdir(self.path):\n",
        "            if \".csv\" in f:\n",
        "                continue\n",
        "\n",
        "            # move-on only if valid datapoint, i.e. has a transcript\n",
        "            iid = int(f.split('.')[0])\n",
        "            if iid not in self.csv.image_id.tolist():\n",
        "                continue\n",
        "            imageIDs.append(iid); tot += 1\n",
        "\n",
        "            # image\n",
        "            ig = cv2.imread(os.path.join(self.path, f))\n",
        "            ig = cv2.resize(ig, (224, 224), interpolation = cv2.INTER_CUBIC)\n",
        "            ig = torch.tensor(ig.reshape(1, 3, 224, 224)) / ig.max()\n",
        "            ims.append(ig)\n",
        "            mu += ig.mean([2, 3]).reshape((1,3,1,1))\n",
        "            sigma += ig.std([2, 3]).reshape((1,3,1,1))\n",
        "        mu /= tot\n",
        "        sigma /= tot\n",
        "\n",
        "        tr = 0\n",
        "        for iid, ig in zip(imageIDs, ims):\n",
        "            # image\n",
        "            ig = (ig - mu) / sigma\n",
        "            ims[tr] = ig\n",
        "            tr += 1\n",
        "\n",
        "            #label\n",
        "            l = self.csv[self.csv.image_id == iid][\"labels\"].astype(float)\n",
        "            labs.append(l)\n",
        "\n",
        "            # transcripts\n",
        "            s = self.csv[self.csv.image_id == iid][\"tokens\"].tolist()[0]\n",
        "            scpts.append(s)\n",
        "\n",
        "            # masks\n",
        "            m = self.csv[self.csv.image_id == iid][\"masks\"].tolist()[0]\n",
        "            ms.append(m)\n",
        "\n",
        "            if int(l.iloc[0]) == 1 and mode == \"train\":\n",
        "                labs.extend([l,] * 3)\n",
        "                scpts.extend([s,] * 3)\n",
        "                ms.extend([m,] * 3)\n",
        "                ims.extend([ig,] * 3)\n",
        "\n",
        "        self.images = torch.cat(ims, dim = 0)\n",
        "        self.labels = labs\n",
        "        self.tokens = scpts\n",
        "        self.masks = ms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        return self.images[ix].to(torch.float16), torch.tensor(self.labels[ix].item()).to(torch.float16), torch.tensor(self.tokens[ix]), torch.tensor(self.masks[ix]).to(torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c576a8b0",
      "metadata": {
        "id": "c576a8b0"
      },
      "outputs": [],
      "source": [
        "\"\"\" Load datasets using class above \"\"\"\n",
        "train_data = LanData(\"train\", dpth)\n",
        "test_data = LanData(\"test\", dpth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29d4b31",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "d29d4b31"
      },
      "outputs": [],
      "source": [
        "\"\"\" Creat pytorch dataloaders from nn.dataset classes above \"\"\"\n",
        "train_loader = DataLoader(train_data, batch_size = N, shuffle = True, drop_last = True)\n",
        "test_loader = DataLoader(test_data, batch_size = N, shuffle = True, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccf58f0",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1ccf58f0"
      },
      "outputs": [],
      "source": [
        "class ImageModel(nn.Module):\n",
        "    def __init__(self, vit):\n",
        "        super(ImageModel, self).__init__()\n",
        "        vit.head = nn.Linear(768, 128)\n",
        "        self.embedding = vit\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6fad53",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7a6fad53"
      },
      "outputs": [],
      "source": [
        "class TextModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Text classification model using llama\n",
        "    Keep llama except the final decoding layer\n",
        "    Add linear layers to transform into a lower dimensional embedding\n",
        "    Add classification head\n",
        "    \"\"\"\n",
        "    def __init__(self, llama):\n",
        "        \"\"\" Pass in the loaded pre-trained llama \"\"\"\n",
        "        super(TextModel, self).__init__()\n",
        "        self.text_base = llama.model\n",
        "        self.clf_head = nn.Sequential(\n",
        "            nn.Linear(4096, 512, bias = False),\n",
        "            nn.Dropout(0.5, inplace = False),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Linear(512, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        text_hidden = self.text_base(x, attention_mask = mask).last_hidden_state.mean(1)\n",
        "        clf_out = self.clf_head(text_hidden)\n",
        "\n",
        "        return clf_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30573b2a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "30573b2a"
      },
      "outputs": [],
      "source": [
        "class TextEmbedding(nn.Module):\n",
        "    def __init__(self, text_model):\n",
        "        super(TextEmbedding, self).__init__()\n",
        "        text_model.clf_head[4] = nn.Linear(512, 128, bias = False)\n",
        "        self.embedding = text_model\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        return self.embedding(x, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b934ea9e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b934ea9e"
      },
      "outputs": [],
      "source": [
        "class Ensemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Receives combined logits from both image and text model\n",
        "    passed to L1 : a linear layer (4 parameters)\n",
        "    then sigmoid output for final decision logit\n",
        "    \"\"\"\n",
        "    def __init__(self, fusion = \"concat\"):\n",
        "        super(Ensemble, self).__init__()\n",
        "        self.fusion = fusion\n",
        "        if self.fusion == \"concat\":\n",
        "            self.d = 256\n",
        "        elif self.fusion == \"element\":\n",
        "            self.d = 128\n",
        "        elif self.fusion == \"sumpool\":\n",
        "            self.w1 = nn.Linear(128, 64, bias = False)\n",
        "            self.w2 = nn.Linear(128, 64, bias = False)\n",
        "            self.sumpool = nn.AvgPool1d(4)\n",
        "            self.d = 16\n",
        "        elif self.fusion == \"gated\":\n",
        "            self.U = nn.Linear(128, 64, bias = False)\n",
        "            self.sig = nn.Sigmoid()\n",
        "            self.V = nn.Linear(128, 64, bias = False)\n",
        "            self.W = nn.Linear(64, 8, bias = False)\n",
        "            self.d = 8\n",
        "\n",
        "        self.initial = nn.Linear(self.d, self.d // 2, bias = True)\n",
        "        self.final = nn.Linear(self.d // 2, 1, bias = True)\n",
        "        self.act_out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, d1, d2):\n",
        "        if self.fusion == \"concat\":\n",
        "            fused = torch.concat([d1, d2], dim = -1)\n",
        "        elif self.fusion == \"element\":\n",
        "            fused = d1 * d2\n",
        "        elif self.fusion == \"sumpool\":\n",
        "            venc = self.w1(d1)\n",
        "            tenc = self.w2(d2)\n",
        "            fused = self.sumpool(venc * tenc)\n",
        "        elif self.fusion == \"gated\":\n",
        "            tenc = self.sig(self.U(d1))\n",
        "            venc = self.V(d2)\n",
        "            enc = tenc * venc\n",
        "            fused = self.W(enc)\n",
        "        hidden = self.initial(fused)\n",
        "        out = self.final(hidden)\n",
        "\n",
        "        if self.training == False:\n",
        "            out = self.act_out(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebde1f0",
      "metadata": {
        "id": "cebde1f0"
      },
      "outputs": [],
      "source": [
        "for fusion_method in [\"gated\"]:  #\"concat\",\"element\", \"sumpool\",\n",
        "    for elr in [1e-5]: #, 2e-5, 3e-5, 4e-5,5e-5\n",
        "        print(\"beginning training \", fusion_method, \" with learning rate \", elr)\n",
        "        print(\"lr = \", elr)\n",
        "        # fusion_method = \"gated\"\n",
        "        llama_clf = TextModel(model).half().to(device)\n",
        "        vit_clf = img_model.half().to(device)\n",
        "        stacker = Ensemble(fusion = fusion_method).half().to(device)\n",
        "\n",
        "        #load pre-trained weights here\n",
        "    #     llama_clf.load_state_dict(torch.load(\"codes/multimodal/models/try/tamilllama/model.bin\"))\n",
        "    # #         # load vit here\n",
        "    #     vit_clf.load_state_dict(torch.load(\".../path/...\"))\n",
        "        #load pre-trained weights here\n",
        "\n",
        "        llama_clf = TextEmbedding(llama_clf).half().to(device)\n",
        "        vit_clf = ImageModel(vit_clf).half().to(device)\n",
        "\n",
        "        # freeze llama pre-trained weights\n",
        "        for p in llama_clf.embedding.text_base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        for p in llama_clf.embedding.clf_head.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # freeze vit pre-trained weights\n",
        "        for p in vit_clf.embedding.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        for p in vit_clf.embedding.head.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # set training to True for ensemble module\n",
        "        for p in stacker.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # list of trainable parameters to pass to optimizer\n",
        "        params = list(llama_clf.embedding.clf_head.parameters()) + list(vit_clf.embedding.head.parameters()) + list(stacker.parameters())\n",
        "\n",
        "        # SGD optimizer\n",
        "        optimizer = torch.optim.Adam(params, lr = elr, eps = 1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 0, threshold = 1e-2)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        llama_clf.train()\n",
        "        vit_clf.train()\n",
        "        stacker.train()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            its = 0\n",
        "            epoch_loss = 0\n",
        "            best_loss = 1_000_000.\n",
        "\n",
        "            with tqdm(train_loader, unit = \"batch\") as train_epoch:\n",
        "                for img, lab, text, mask in train_epoch:\n",
        "                    img = img.to(device)\n",
        "                    lab = lab.to(device)\n",
        "                    text = text.to(device)\n",
        "                    mask = mask.to(device)\n",
        "                    train_epoch.set_description(f\"Epoch {epoch}\")\n",
        "                    batch_loss = 0\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    d1 = llama_clf(text, mask)\n",
        "                    d2 = vit_clf(img)\n",
        "\n",
        "                    out = stacker(d1, d2).squeeze(1)\n",
        "                    batch_loss = criterion(out, lab)\n",
        "\n",
        "                    batch_loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    epoch_loss += batch_loss.item()\n",
        "                    its += 1\n",
        "\n",
        "            scheduler.step(epoch_loss)\n",
        "            epoch_loss /= its\n",
        "\n",
        "            print(\"Average epoch loss: \", epoch_loss)\n",
        "\n",
        "        # testing\n",
        "        llama_clf.eval()\n",
        "        vit_clf.eval()\n",
        "        stacker.eval()\n",
        "        ths = [x/10 for x in range(1, 10)] # 0.1 - 1\n",
        "        accs = []\n",
        "\n",
        "        for th in ths:\n",
        "            test_loss = 0\n",
        "            its = 0\n",
        "            correct = 0\n",
        "\n",
        "            with tqdm(test_loader, unit = \"batch\") as test_epoch:\n",
        "                for img, lab, text, mask in test_epoch:\n",
        "                    img, lab, text, mask = img.to(device), lab.to(device), text.to(device), mask.to(device)\n",
        "                    test_epoch.set_description(f\"Testing...\")\n",
        "                    batch_loss = 0\n",
        "\n",
        "                    d1 = llama_clf(text, mask)\n",
        "                    d2 = vit_clf(img)\n",
        "\n",
        "                    out = stacker(d1, d2).squeeze(1)\n",
        "\n",
        "                    pred = (out > th).to(torch.float)\n",
        "                    correct += (pred == lab).sum()\n",
        "\n",
        "                    batch_loss = criterion(out, lab)\n",
        "\n",
        "                    test_loss += batch_loss.item()\n",
        "                    its += 1\n",
        "\n",
        "            test_loss /= its\n",
        "            accuracy = (correct / (its * 16)).item() # its --> number of items ;  16 --> batch size\n",
        "\n",
        "            accs.append(accuracy)\n",
        "\n",
        "            print(f\"Accuracy: {accuracy * 100:.2f} %\")\n",
        "        print(f\"Average test set loss:  {test_loss:.5f}\")\n",
        "\n",
        "        best_thr = ths[accs.index(max(accs))]\n",
        "        print(f\"Best threshold computed at: {best_thr}\")\n",
        "\n",
        "        print(\"Now producing results at best threshold:-\")\n",
        "\n",
        "        preds = []\n",
        "        tru = []\n",
        "\n",
        "        test_loss = 0\n",
        "        its = 0\n",
        "        correct = 0\n",
        "\n",
        "        with tqdm(test_loader, unit = \"batch\") as test_epoch:\n",
        "            for img, lab, text, mask in test_epoch:\n",
        "                img, lab, text, mask = img.to(device), lab.to(device), text.to(device), mask.to(device)\n",
        "                test_epoch.set_description(f\"Testing...\")\n",
        "                batch_loss = 0\n",
        "\n",
        "                d1 = llama_clf(text, mask)\n",
        "                d2 = vit_clf(img)\n",
        "\n",
        "                out = stacker(d1, d2).squeeze(1)\n",
        "\n",
        "                pred = (out > best_thr).to(torch.float)\n",
        "                correct += (pred == lab).sum()\n",
        "\n",
        "                batch_loss = criterion(out, lab)\n",
        "\n",
        "                test_loss += batch_loss.item()\n",
        "                its += 1\n",
        "                preds.extend(pred.tolist())\n",
        "                tru.extend(lab.tolist())\n",
        "\n",
        "        test_loss /= its\n",
        "        accuracy = (correct / (its * 16)).item()\n",
        "\n",
        "        print(f\"Accuracy: {accuracy * 100:.2f} %\")\n",
        "        print(f\"Average test set loss:  {test_loss:.5f}\")\n",
        "\n",
        "\n",
        "        # plot confusion matrix\n",
        "        ConfusionMatrixDisplay(confusion_matrix(tru, preds)).plot()\n",
        "\n",
        "        # create directories to save models and metrices\n",
        "        # save_model_path = \"saved_models/\"+str(language)+\"/text/tamilllama/\"\n",
        "        save_confusion_matrix_path = \"predictions/\"+str(language)+\"/fusion/\"\n",
        "        metrices_path = \"predictions/\"+str(language)+\"/fusion/\"\n",
        "\n",
        "\n",
        "        # isExist = os.path.exists(save_model_path)\n",
        "        # if not isExist:\n",
        "        #     # Create a new directory because it does not exist\n",
        "        #     os.makedirs(save_model_path)\n",
        "        isExist = os.path.exists(save_confusion_matrix_path)\n",
        "        if not isExist:\n",
        "            # Create a new directory because it does not exist\n",
        "            os.makedirs(save_confusion_matrix_path)\n",
        "        isExist = os.path.exists(metrices_path)\n",
        "        if not isExist:\n",
        "            # Create a new directory because it does not exist\n",
        "            os.makedirs(metrices_path)\n",
        "        # save predictions\n",
        "        preds_df = pd.DataFrame({'predictions':preds})\n",
        "        preds_df.to_csv(save_confusion_matrix_path+\"custom_\"+str(language)+\"_llamavit_fusionsumpool_pretrained_\"+str(elr)+\"_\"+str(epochs)+\"_\"+str(N)+\".csv\", index = False)\n",
        "\n",
        "        # saving confusion matrix plot\n",
        "        # plt.savefig(save_confusion_matrix_path+\"custom_\"+str(language)+\"_llamavit_fusionsumpool_pretrained_\"+str(elr)+\"_\"+str(epochs)+\"_\"+str(N)+\".png\")\n",
        "        print(classification_report(tru, preds, digits = 5))\n",
        "\n",
        "        cf = confusion_matrix(tru, preds)\n",
        "        cr = classification_report(tru, preds, digits = 5)\n",
        "        # saving metrices and confussion matrix\n",
        "        with open(metrices_path+\"custom_\"+str(language)+\"llama_vit_\"+fusion_method+\"fusion_\"+str(N)+\".txt\", \"a\") as text_file:\n",
        "            text_file.write(\"Date and time of run = \"+str(dt)+\"\\n\")\n",
        "            text_file.write(str(elr)+\"_\"+str(epochs)+\"_\"+str(N)+\"\\n\\n\")\n",
        "            text_file.write(str(cf)+\"\\n\")\n",
        "            text_file.write(cr)\n",
        "            text_file.write(\"\\n\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}